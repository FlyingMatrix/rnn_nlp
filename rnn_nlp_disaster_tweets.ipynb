{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN for NLP - Disaster Tweets Analysis\n",
    "# https://www.kaggle.com/c/nlp-getting-started\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train data\n",
    "df = pd.read_csv(\"./data/train.csv\")\n",
    "print(df.shape)\n",
    "# Show samples of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3271\n",
      "4342\n"
     ]
    }
   ],
   "source": [
    "# Data summary\n",
    "print((df.target == 1).sum()) # Disaster\n",
    "print((df.target == 0).sum()) # No Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Function for removing url\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# Function for removing punctuation\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my', 'wouldn', 'with', 'not', \"needn't\", 'by', \"couldn't\", 'isn', 'couldn', 'how', \"mightn't\", 'is', \"didn't\", 'very', 'during', 'doesn', 'hadn', 'weren', 'we', 'or', 'who', 'there', 'to', 'was', 'they', 'didn', 'that', 'o', 'did', 'when', 'hasn', 'his', 'any', \"weren't\", 'from', 'her', 'below', 'further', 're', 'so', 'haven', \"should've\", 'had', 'only', 'yourself', 'than', 'll', 'shouldn', \"wasn't\", 'now', 'won', \"you're\", 'at', 'shan', 'into', 'most', 'which', 'been', 'other', 'because', \"won't\", 'he', 'mightn', \"you'll\", \"it's\", 'of', 'through', \"don't\", \"you'd\", 'once', 'him', 'myself', \"haven't\", 'all', 'being', 'each', 'few', 'some', 'your', 'hers', 'doing', 'over', 'here', 'just', 'them', 'more', 'its', 'after', 'd', 'm', 'on', 'again', 'whom', 'wasn', 't', \"doesn't\", \"mustn't\", 'herself', 'am', 'it', 'don', 'against', 'aren', 'were', 'yourselves', \"hasn't\", \"wouldn't\", 'those', 'until', 'both', \"you've\", 'ourselves', 'mustn', 'what', 'the', 'our', 'their', 'under', 'these', 'do', 'off', 'ma', 'are', 'i', 'me', 'about', 'a', \"that'll\", \"hadn't\", 'ain', 'this', 'can', 'does', 'but', \"shan't\", 'should', 'you', 'have', 'has', 'before', 'will', 'between', 'up', \"she's\", 'she', \"shouldn't\", 've', 'if', 'out', 'himself', 'needn', 'ours', 'yours', 'down', 'above', 'same', 'an', 'be', 's', \"aren't\", 'while', 'y', 'no', 'such', 'theirs', 'having', \"isn't\", 'too', 'and', 'for', 'where', 'as', 'own', 'themselves', 'nor', 'in', 'why', 'then', 'itself'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\7000028508\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "print(stop)\n",
    "\n",
    "# Function for removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Our Deeds are the Reason of this #earthquake M...\n",
      "1                  Forest fire near La Ronge Sask. Canada\n",
      "2       All residents asked to 'shelter in place' are ...\n",
      "3       13,000 people receive #wildfires evacuation or...\n",
      "4       Just got sent this photo from Ruby #Alaska as ...\n",
      "                              ...                        \n",
      "7608    Two giant cranes holding a bridge collapse int...\n",
      "7609    @aria_ahrary @TheTawniest The out of control w...\n",
      "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
      "7611    Police investigating after an e-bike collided ...\n",
      "7612    The Latest: More Homes Razed by Northern Calif...\n",
      "Name: text, Length: 7613, dtype: object\n",
      "0       Our Deeds are the Reason of this #earthquake M...\n",
      "1                  Forest fire near La Ronge Sask. Canada\n",
      "2       All residents asked to 'shelter in place' are ...\n",
      "3       13,000 people receive #wildfires evacuation or...\n",
      "4       Just got sent this photo from Ruby #Alaska as ...\n",
      "                              ...                        \n",
      "7608    Two giant cranes holding a bridge collapse int...\n",
      "7609    @aria_ahrary @TheTawniest The out of control w...\n",
      "7610          M1.94 [01:04 UTC]?5km S of Volcano Hawaii. \n",
      "7611    Police investigating after an e-bike collided ...\n",
      "7612    The Latest: More Homes Razed by Northern Calif...\n",
      "Name: text, Length: 7613, dtype: object\n",
      "0       Our Deeds are the Reason of this earthquake Ma...\n",
      "1                   Forest fire near La Ronge Sask Canada\n",
      "2       All residents asked to shelter in place are be...\n",
      "3       13000 people receive wildfires evacuation orde...\n",
      "4       Just got sent this photo from Ruby Alaska as s...\n",
      "                              ...                        \n",
      "7608    Two giant cranes holding a bridge collapse int...\n",
      "7609    ariaahrary TheTawniest The out of control wild...\n",
      "7610                M194 0104 UTC5km S of Volcano Hawaii \n",
      "7611    Police investigating after an ebike collided w...\n",
      "7612    The Latest More Homes Razed by Northern Califo...\n",
      "Name: text, Length: 7613, dtype: object\n",
      "0            deeds reason earthquake may allah forgive us\n",
      "1                   forest fire near la ronge sask canada\n",
      "2       residents asked shelter place notified officer...\n",
      "3       13000 people receive wildfires evacuation orde...\n",
      "4       got sent photo ruby alaska smoke wildfires pou...\n",
      "                              ...                        \n",
      "7608    two giant cranes holding bridge collapse nearb...\n",
      "7609    ariaahrary thetawniest control wild fires cali...\n",
      "7610                      m194 0104 utc5km volcano hawaii\n",
      "7611    police investigating ebike collided car little...\n",
      "7612    latest homes razed northern california wildfir...\n",
      "Name: text, Length: 7613, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove url, punctuation and stopwords from training data\n",
    "print(df.text)\n",
    "\n",
    "df[\"text\"] = df.text.map(remove_URL) # map(lambda x: remove_URL(x))\n",
    "print(df.text)\n",
    "\n",
    "df[\"text\"] = df.text.map(remove_punct)\n",
    "print(df.text)\n",
    "\n",
    "df[\"text\"] = df.text.map(remove_stopwords)\n",
    "print(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17971\n",
      "[('like', 345), ('im', 299), ('amp', 298), ('fire', 250), ('get', 229)]\n"
     ]
    }
   ],
   "source": [
    "# Count unique words\n",
    "from collections import Counter\n",
    "\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "counter = counter_word(df.text)\n",
    "num_unique_words = len(counter)\n",
    "\n",
    "print(num_unique_words)\n",
    "print(counter.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090,) (1523,)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and validation set\n",
    "train_size = int(df.shape[0] * 0.8)\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:]\n",
    "\n",
    "# Split text and labels\n",
    "train_sentences = train_df.text.to_numpy()\n",
    "train_labels = train_df.target.to_numpy()\n",
    "\n",
    "val_sentences = val_df.text.to_numpy()\n",
    "val_labels = val_df.target.to_numpy()\n",
    "\n",
    "print(train_sentences.shape, val_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training\n",
    "\n",
    "# each word has unique index\n",
    "word_index = tokenizer.word_index\n",
    "# print(word_index)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
    "\n",
    "# print(train_sentences[10:15])\n",
    "# print(train_sequences[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 20) (1523, 20)\n"
     ]
    }
   ],
   "source": [
    "# Pad the sequences to have the same length\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Max number of words in a sequence\n",
    "max_length = 20\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "print(train_padded.shape, val_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome time visiting cfc head office ancop site ablaze thanks tita vida taking care us\n",
      "[751, 33, 5695, 3752, 313, 1074, 5696, 803, 375, 454, 5697, 3753, 896, 397, 14]\n",
      "[ 751   33 5695 3752  313 1074 5696  803  375  454 5697 3753  896  397\n",
      "   14    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Sample\n",
    "print(train_sentences[42])\n",
    "print(train_sequences[42])\n",
    "print(train_padded[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the indices - flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n",
    "# print(reverse_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[751, 33, 5695, 3752, 313, 1074, 5696, 803, 375, 454, 5697, 3753, 896, 397, 14]\n",
      "awesome time visiting cfc head office ancop site ablaze thanks tita vida taking care us\n"
     ]
    }
   ],
   "source": [
    "# Decode from sequence: sequence -> sentence\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "\n",
    "decoded_text = decode(train_sequences[42])\n",
    "\n",
    "print(train_sequences[42])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 32)            575072    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 8)                 1312      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 576,393\n",
      "Trainable params: 576,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Embedding: Turns positive integers (indexes) into dense vectors of fixed size. \n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have \n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "# -> (batch_size, input_length, output_dim) = (batch_size, 20, 32)\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\n",
    "\n",
    "model.add(layers.LSTM(8, dropout=0.4))\n",
    "# -> (batch_size, output_dim) = (batch_size, 8)\n",
    "\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\")) # value range in [0, 1]\n",
    "# -> (batch_size, output_dim) = (batch_size, 1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Loss, optim and metrics\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False) # for binary classification\n",
    "optim = keras.optimizers.Adam(learning_rate=0.0002)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "191/191 - 4s - loss: 0.6864 - accuracy: 0.5511 - val_loss: 0.6910 - val_accuracy: 0.5345 - 4s/epoch - 20ms/step\n",
      "Epoch 2/10\n",
      "191/191 - 2s - loss: 0.6723 - accuracy: 0.5793 - val_loss: 0.6852 - val_accuracy: 0.5345 - 2s/epoch - 9ms/step\n",
      "Epoch 3/10\n",
      "191/191 - 2s - loss: 0.5758 - accuracy: 0.6869 - val_loss: 0.5396 - val_accuracy: 0.7971 - 2s/epoch - 10ms/step\n",
      "Epoch 4/10\n",
      "191/191 - 2s - loss: 0.3926 - accuracy: 0.8639 - val_loss: 0.5134 - val_accuracy: 0.7800 - 2s/epoch - 10ms/step\n",
      "Epoch 5/10\n",
      "191/191 - 2s - loss: 0.3151 - accuracy: 0.8949 - val_loss: 0.5240 - val_accuracy: 0.7728 - 2s/epoch - 10ms/step\n",
      "Epoch 6/10\n",
      "191/191 - 2s - loss: 0.2618 - accuracy: 0.9156 - val_loss: 0.5392 - val_accuracy: 0.7787 - 2s/epoch - 10ms/step\n",
      "Epoch 7/10\n",
      "191/191 - 2s - loss: 0.2188 - accuracy: 0.9365 - val_loss: 0.5577 - val_accuracy: 0.7781 - 2s/epoch - 9ms/step\n",
      "Epoch 8/10\n",
      "191/191 - 2s - loss: 0.1936 - accuracy: 0.9458 - val_loss: 0.5851 - val_accuracy: 0.7754 - 2s/epoch - 9ms/step\n",
      "Epoch 9/10\n",
      "191/191 - 2s - loss: 0.1715 - accuracy: 0.9519 - val_loss: 0.6250 - val_accuracy: 0.7682 - 2s/epoch - 9ms/step\n",
      "Epoch 10/10\n",
      "191/191 - 2s - loss: 0.1583 - accuracy: 0.9558 - val_loss: 0.6174 - val_accuracy: 0.7787 - 2s/epoch - 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1324fdc3dc0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the training\n",
    "model.fit(train_padded, train_labels, epochs=10, validation_data=(val_padded, val_labels), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(\"./data/test.csv\")\n",
    "print(df.shape)\n",
    "# Show samples of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove url, punctuation and stopwords from test data\n",
    "df[\"text\"] = df.text.map(remove_URL)\n",
    "df[\"text\"] = df.text.map(remove_punct)\n",
    "df[\"text\"] = df.text.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test_sentences\n",
    "test_sentences = df.text.to_numpy()\n",
    "\n",
    "# Tokenization\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Padding\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions on the test data\n",
    "predictions = model.predict(test_padded)\n",
    "predictions = [1 if p > 0.5 else 0 for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets: happened terrible car crash\n",
      "Result: Disaster\n",
      "===================================================================\n",
      "Tweets: heard earthquake different cities stay safe everyone\n",
      "Result: Disaster\n",
      "===================================================================\n",
      "Tweets: forest fire spot pond geese fleeing across street cannot save\n",
      "Result: Disaster\n",
      "===================================================================\n",
      "Tweets: apocalypse lighting spokane wildfires\n",
      "Result: Disaster\n",
      "===================================================================\n",
      "Tweets: typhoon soudelor kills 28 china taiwan\n",
      "Result: Disaster\n",
      "===================================================================\n",
      "Tweets: shakingits earthquake\n",
      "Result: Disaster\n",
      "===================================================================\n",
      "Tweets: theyd probably still show life arsenal yesterday eh eh\n",
      "Result: No disaster\n",
      "===================================================================\n",
      "Tweets: hey\n",
      "Result: No disaster\n",
      "===================================================================\n",
      "Tweets: nice hat\n",
      "Result: No disaster\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "# Sample results of the predictions on the test data\n",
    "for idx in range(0, 9):\n",
    "    print(f\"Tweets: \" + df.text[idx])\n",
    "    print(f\"Result: \" + \"Disaster\" if(predictions[idx] == 1) else f\"Result: \" + \"No disaster\") \n",
    "    # 1 -> Disaster, 2-> No disaster\n",
    "    print(\"===================================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
